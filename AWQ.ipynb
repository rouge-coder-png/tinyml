{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acd2e19",
   "metadata": {},
   "source": [
    "## Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc413ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization config: {'zero_point': True, 'q_group_size': 128}\n",
      "* Building model /PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:17<00:00,  8.84s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      " * Split into 65 blocks\n",
      "Running AWQ...: 100%|███████████████████████████| 32/32 [17:32<00:00, 32.91s/it]\n",
      "AWQ results saved at awq_cache/openchat_3.5-w4-g128.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m awq.entry --model_path ~/.cache/huggingface/transformers/openchat_3.5  \\\n",
    "    --w_bit 4 --q_group_size 128 \\\n",
    "    --run_awq --dump_awq awq_cache/openchat_3.5-w4-g128.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a3c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization config: {'zero_point': True, 'q_group_size': 128}\n",
      "* Building model /PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:20<00:00, 10.02s/it]\n",
      "Loading pre-computed AWQ results from awq_cache/openchat_3.5-w4-g128.pt\n",
      "real weight quantization...: 100%|██████████████| 32/32 [04:09<00:00,  7.80s/it]\n",
      "Saving the quantized model at quant_cache/openchat_3.5-w4-g128.pt...\n"
     ]
    }
   ],
   "source": [
    "!python -m awq.entry --model_path ~/.cache/huggingface/transformers/openchat_3.5 \\\n",
    "    --w_bit 4 --q_group_size 128 \\\n",
    "    --load_awq awq_cache/openchat_3.5-w4-g128.pt \\\n",
    "    --q_backend real --dump_quant quant_cache/openchat_3.5-w4-g128.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423c3d2",
   "metadata": {},
   "source": [
    "## Load the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220daecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator, tasks\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model, load_checkpoint_in_model\n",
    "from accelerate.utils.modeling import get_balanced_memory\n",
    "from awq.utils.parallel import auto_parallel\n",
    "from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "from awq.quantize.quantizer import pseudo_quantize_model_weight, real_quantize_model_weight\n",
    "from awq.utils.lm_eval_adaptor import LMEvalAdaptor\n",
    "from awq.utils.utils import simple_dispatch_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eabb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e572a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_config = {\n",
    "    \"zero_point\": True,  # by default True\n",
    "    \"q_group_size\": 128,  # whether to use group quantization\n",
    "}\n",
    "max_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e26345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_enc(model_path, quantized_file_path, load_quant = True, w_bit = 4):\n",
    "    if not os.path.exists(model_path):  # look into ssd\n",
    "        raise FileNotFoundError(f\"{model_path} not found!\")\n",
    "    print(f\"* Building model {model_path}\")\n",
    "\n",
    "    # all hf model\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    print(f\"Config = {config}\")\n",
    "    if \"mpt\" in config.__class__.__name__.lower():\n",
    "        enc = AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=True)\n",
    "    else:\n",
    "        enc = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "    if load_quant:  # directly load quantized weights\n",
    "        print(\"Loading pre-computed quantized weights...\")\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config=config,\n",
    "                                                     torch_dtype=torch.float16, trust_remote_code=True)\n",
    "        model.config.pretraining_tp = 1\n",
    "        real_quantize_model_weight(\n",
    "            model, w_bit=w_bit, q_config=q_config, init_only=True)\n",
    "        \n",
    "        model.tie_weights()\n",
    "        \n",
    "        # Infer device map\n",
    "        kwargs = {\"max_memory\": max_memory} if len(max_memory) else {}\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            no_split_module_classes=[\n",
    "                \"OPTDecoderLayer\", \"LlamaDecoderLayer\", \"BloomBlock\", \"MPTBlock\", \"DecoderLayer\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        # Load checkpoint in the model\n",
    "        load_checkpoint_in_model(\n",
    "            model,\n",
    "            checkpoint= quantized_file_path,\n",
    "            device_map=device_map,\n",
    "            offload_state_dict=True,\n",
    "        )\n",
    "        # Dispatch model\n",
    "        model = simple_dispatch_model(model, device_map=device_map)\n",
    "\n",
    "        model.eval()\n",
    "    else:  # fp16 to quantized\n",
    "        args.run_awq &= not args.load_awq  # if load_awq, no need to run awq\n",
    "        # Init model on CPU:\n",
    "        kwargs = {\"torch_dtype\": torch.float16, \"low_cpu_mem_usage\": True}\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, config=config, trust_remote_code=True, **kwargs)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if args.run_awq:\n",
    "            assert args.dump_awq, \"Please save the awq results with --dump_awq\"\n",
    "                        \n",
    "            awq_results = run_awq(\n",
    "                model, enc,\n",
    "                w_bit=args.w_bit, q_config=q_config,\n",
    "                n_samples=128, seqlen=512,\n",
    "            )\n",
    "            if args.dump_awq:\n",
    "                dirpath = os.path.dirname(args.dump_awq)\n",
    "                os.makedirs(dirpath, exist_ok=True)\n",
    "                \n",
    "                torch.save(awq_results, args.dump_awq)\n",
    "                print(\"AWQ results saved at\", args.dump_awq)\n",
    "                \n",
    "            exit(0)\n",
    "                \n",
    "        if args.load_awq:\n",
    "            print(\"Loading pre-computed AWQ results from\", args.load_awq)\n",
    "            awq_results = torch.load(args.load_awq, map_location=\"cpu\")\n",
    "            apply_awq(model, awq_results)\n",
    "\n",
    "        # weight quantization\n",
    "        if args.w_bit is not None:\n",
    "            if args.q_backend == \"fake\":\n",
    "                assert args.dump_quant is None, \\\n",
    "                    \"Need to use real quantization to dump quantized weights\"\n",
    "                pseudo_quantize_model_weight(\n",
    "                    model, w_bit=args.w_bit, q_config=q_config\n",
    "                )\n",
    "            elif args.q_backend == \"real\":  # real quantization\n",
    "                real_quantize_model_weight(\n",
    "                    model, w_bit=args.w_bit, q_config=q_config\n",
    "                )\n",
    "                if args.dump_quant:\n",
    "                    dirpath = os.path.dirname(args.dump_quant)\n",
    "                    os.makedirs(dirpath, exist_ok=True)\n",
    "                    \n",
    "                    print(\n",
    "                        f\"Saving the quantized model at {args.dump_quant}...\")\n",
    "                    torch.save(model.cpu().state_dict(), args.dump_quant)\n",
    "                    exit(0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "        # Move the model to GPU (as much as possible) for LM evaluation\n",
    "        kwargs = {\"max_memory\": get_balanced_memory(model, max_memory if len(max_memory) > 0 else None)}\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            # TODO: can we remove this?\n",
    "            no_split_module_classes=[\n",
    "                \"OPTDecoderLayer\", \"LlamaDecoderLayer\", \"BloomBlock\", \"MPTBlock\", \"DecoderLayer\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "    return model, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5436e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building model ../.cache/huggingface/transformers/openchat_3.5\n",
      "Config = MistralConfig {\n",
      "  \"_name_or_path\": \"../.cache/huggingface/transformers/openchat_3.5\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "Loading pre-computed quantized weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "real weight quantization...(init only): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 838.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../.cache/huggingface/transformers/openchat_3.5\"\n",
    "quantized_file_path = \"quant_cache/openchat_3.5-w4-g128.pt\"\n",
    "model, tokenizer = build_model_and_enc(model_path, quantized_file_path = quantized_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab0f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): WQLinear(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
       "          (k_proj): WQLinear(in_features=4096, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
       "          (v_proj): WQLinear(in_features=4096, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
       "          (o_proj): WQLinear(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): WQLinear(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
       "          (up_proj): WQLinear(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
       "          (down_proj): WQLinear(in_features=14336, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febb5f5",
   "metadata": {},
   "source": [
    "## Test the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6bf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=1024)\n",
    "    print(generate_ids)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64e2971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1824,   349,  ..., 28723,    13,    13]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What is the difference between Harvard and MIT?\\n\\nHarvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While both schools are highly regarded for their academic programs, there are several key differences between the two.\\n\\n1. Location: Harvard University is located in Cambridge, Massachusetts, while MIT is also located in Cambridge, Massachusetts. Both schools are part of the Cambridge-Boston metropolitan area, which is known for its rich history, culture, and academic institutions.\\n\\n2. Size: Harvard University has a larger student body than MIT, with approximately 20,000 students enrolled, compared to MIT's approximately 11,000 students. This means that Harvard has a more diverse student population and a wider range of academic programs and extracurricular activities.\\n\\n3. Academic Focus: Harvard University is a liberal arts college, which means it offers a broad range of academic programs in the humanities, social sciences, and natural sciences. MIT, on the other hand, is a research university with a focus on science, technology, engineering, and mathematics (STEM) fields.\\n\\n4. Admissions: Harvard University has a more selective admissions process than MIT, with an acceptance rate of around 5% compared to MIT's acceptance rate of around 7%. This means that it is generally more difficult to gain admission to Harvard than to MIT.\\n\\n5. Tuition: Harvard University and MIT both have high tuition fees, but MIT's tuition is generally higher than Harvard's. However, both schools offer generous financial aid packages to help students cover the cost of attendance.\\n\\n6. Rankings: Both Harvard University and MIT are consistently ranked among the top universities in the world. Harvard is often ranked higher in the overall rankings, while MIT is often ranked higher in the fields of science, technology, engineering, and mathematics.\\n\\nIn summary, Harvard University and MIT are both highly respected institutions of higher learning, but they differ in terms of size, academic focus, admissions, tuition, and rankings. Prospective students should consider these factors when deciding which school is the best fit for them.\\n\\nHarvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While both schools are highly regarded for their academic programs, there are several key differences between the two.\\n\\n1. Location: Harvard University is located in Cambridge, Massachusetts, while MIT is also located in Cambridge, Massachusetts. Both schools are part of the Cambridge-Boston metropolitan area, which is known for its rich history, culture, and academic institutions.\\n\\n2. Size: Harvard University has a larger student body than MIT, with approximately 20,000 students enrolled, compared to MIT's approximately 11,000 students. This means that Harvard has a more diverse student population and a wider range of academic programs and extracurricular activities.\\n\\n3. Academic Focus: Harvard University is a liberal arts college, which means it offers a broad range of academic programs in the humanities, social sciences, and natural sciences. MIT, on the other hand, is a research university with a focus on science, technology, engineering, and mathematics (STEM) fields.\\n\\n4. Admissions: Harvard University has a more selective admissions process than MIT, with an acceptance rate of around 5% compared to MIT's acceptance rate of around 7%. This means that it is generally more difficult to gain admission to Harvard than to MIT.\\n\\n5. Tuition: Harvard University and MIT both have high tuition fees, but MIT's tuition is generally higher than Harvard's. However, both schools offer generous financial aid packages to help students cover the cost of attendance.\\n\\n6. Rankings: Both Harvard University and MIT are consistently ranked among the top universities in the world. Harvard is often ranked higher in the overall rankings, while MIT is often ranked higher in the fields of science, technology, engineering, and mathematics.\\n\\nIn summary, Harvard University and MIT are both highly respected institutions of higher learning, but they differ in terms of size, academic focus, admissions, tuition, and rankings. Prospective students should consider these factors when deciding which school is the best fit for them.\\n\\nHarvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While both schools are highly regarded for their academic programs, there are several key differences between the two.\\n\\n1. Location: Harvard University is located in Cambridge, Massachusetts, while MIT is also located in Cambridge, Massachusetts. Both schools are part of the Cambridge-Boston metropolitan area, which is known for its rich history, culture, and academic institutions.\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the difference between Harvard and MIT?\"\n",
    "quant_start_time = time.time()\n",
    "LLM_text(model, tokenizer, prompt)\n",
    "quant_end_time = time.time()\n",
    "quant_run_time = quant_end_time - quant_start_time\n",
    "print(f\"[INFO]: The quantized model finishes running after {quant_run_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e856b4b",
   "metadata": {},
   "source": [
    "## This is what you will do if you want to load the original, unquantized model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f37b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2979fc439d694c478d1dc36531422b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\").half()\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0486ca",
   "metadata": {},
   "source": [
    "## Test the unquantized model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ba2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=1024)\n",
    "    print(generate_ids)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1644334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1824,   349,  ...,   272, 10539,   659]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What is the difference between Harvard and MIT?\\n\\nHarvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While they are both highly regarded institutions, there are some key differences between the two.\\n\\nHarvard University is a private Ivy League research university located in Cambridge, Massachusetts. It was founded in 1636 and is the oldest institution of higher education in the United States. Harvard is known for its strong liberal arts program, as well as its professional schools in law, business, and medicine. The university offers a wide range of undergraduate and graduate degree programs in various fields of study.\\n\\nMIT, on the other hand, is a private research university located in Cambridge, Massachusetts, and is also known as a leading institution in science, technology, engineering, and mathematics (STEM) fields. MIT was founded in 1861 and is known for its strong emphasis on research and innovation. The university offers a wide range of undergraduate and graduate degree programs in various fields of study, with a particular focus on STEM fields.\\n\\nIn terms of admissions, both Harvard and MIT are highly competitive and selective. Harvard has a lower acceptance rate than MIT, with Harvard's acceptance rate being around 5% and MIT's acceptance rate being around 7%. However, both universities have a strong commitment to financial aid and offer generous financial assistance to students in need.\\n\\nIn terms of campus life, both Harvard and MIT are located in the same city, Cambridge, Massachusetts, and are part of the same community. Both universities have a diverse student body and offer a wide range of extracurricular activities, clubs, and organizations for students to get involved in.\\n\\nOverall, while both Harvard and MIT are highly regarded institutions, they have different strengths and focuses. Harvard is known for its strong liberal arts program and professional schools, while MIT is known for its emphasis on research and innovation in STEM fields. The choice between the two universities will depend on the individual's interests, goals, and preferences.\\n\\nWhat is the difference between Harvard and MIT in terms of academics?\\n\\nHarvard University and the Massachusetts Institute of Technology (MIT) are both highly regarded institutions, but they have different academic focuses and strengths.\\n\\nHarvard University is a private Ivy League research university that offers a wide range of undergraduate and graduate degree programs in various fields of study. Harvard is known for its strong liberal arts program, as well as its professional schools in law, business, and medicine. The university places a strong emphasis on interdisciplinary learning and encourages students to explore multiple fields of study. Harvard's academic programs are designed to provide students with a well-rounded education that prepares them for a variety of careers.\\n\\nMIT, on the other hand, is a private research university that is known for its strong emphasis on research and innovation in science, technology, engineering, and mathematics (STEM) fields. MIT offers a wide range of undergraduate and graduate degree programs in various fields of study, with a particular focus on STEM fields. The university places a strong emphasis on hands-on learning and research, and encourages students to engage in research projects and collaborate with faculty members. MIT's academic programs are designed to prepare students for careers in research, industry, and academia.\\n\\nIn terms of teaching style, Harvard is known for its emphasis on lectures and seminars, while MIT is known for its emphasis on hands-on learning and research. Harvard's teaching style is more focused on discussion and debate, while MIT's teaching style is more focused on problem-solving and experimentation.\\n\\nOverall, the choice between Harvard and MIT in terms of academics will depend on the individual's interests, goals, and preferences. Harvard is a great choice for students who are interested in a well-rounded liberal arts education and professional development, while MIT is a great choice for students who are interested in research and innovation in STEM fields.\\n\\nWhat is the difference between Harvard and MIT in terms of campus life?\\n\\nHarvard University and the Massachusetts Institute of Technology (MIT) are both highly regarded institutions, but they have different campus life experiences.\\n\\nBoth Harvard and MIT are located in the same city, Cambridge, Massachusetts, and are part of the same community. Both universities have a diverse student body and offer a wide range of extracurricular activities, clubs, and organizations for students to get involved in.\\n\\nHarvard's campus life is known for its strong sense of community and tradition. The university has a long history and a beautiful campus, with many historic buildings and landscapes. Harvard's campus is also located in a vibrant city with many cultural and recreational opportunities. Harvard's student body is diverse, with students from all over the world, and the university has\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the difference between Harvard and MIT?\"\n",
    "original_start_time = time.time()\n",
    "LLM_text(model, tokenizer, prompt)\n",
    "original_end_time = time.time()\n",
    "original_run_time = original_end_time - original_start_time\n",
    "print(f\"[INFO]: The original model finishes running after {original_run_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56a7c6",
   "metadata": {},
   "source": [
    "## This is what you will do if you want to load the original, unquantized model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5c7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252fe516ae884996bcab75f19946529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39877244",
   "metadata": {},
   "source": [
    "## Test the unquantized model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be20c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=1024)\n",
    "    print(generate_ids)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a90bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1824,   349,  ..., 10539,   349,  9045]])\n",
      "[INFO]: The original CPU model finishes running after 579.0212705135345 seconds.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the difference between Harvard and MIT?\"\n",
    "original_cpu_start_time = time.time()\n",
    "LLM_text(model, tokenizer, prompt)\n",
    "original_cpu_end_time = time.time()\n",
    "original_cpu_run_time = original_cpu_end_time - original_cpu_start_time\n",
    "print(f\"[INFO]: The original CPU model finishes running after {original_cpu_run_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3116e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
