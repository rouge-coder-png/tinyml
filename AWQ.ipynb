{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acd2e19",
   "metadata": {},
   "source": [
    "## Step 1: Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc413ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization config: {'zero_point': True, 'q_group_size': 128}\n",
      "* Building model /PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:17<00:00,  8.84s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      " * Split into 65 blocks\n",
      "Running AWQ...: 100%|███████████████████████████| 32/32 [17:32<00:00, 32.91s/it]\n",
      "AWQ results saved at awq_cache/openchat_3.5-w4-g128.pt\n"
     ]
    }
   ],
   "source": [
    "!python -m awq.entry --model_path ~/.cache/huggingface/transformers/openchat_3.5  \\\n",
    "    --w_bit 4 --q_group_size 128 \\\n",
    "    --run_awq --dump_awq awq_cache/openchat_3.5-w4-g128.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a3c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization config: {'zero_point': True, 'q_group_size': 128}\n",
      "* Building model /PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:20<00:00, 10.02s/it]\n",
      "Loading pre-computed AWQ results from awq_cache/openchat_3.5-w4-g128.pt\n",
      "real weight quantization...: 100%|██████████████| 32/32 [04:09<00:00,  7.80s/it]\n",
      "Saving the quantized model at quant_cache/openchat_3.5-w4-g128.pt...\n"
     ]
    }
   ],
   "source": [
    "!python -m awq.entry --model_path ~/.cache/huggingface/transformers/openchat_3.5 \\\n",
    "    --w_bit 4 --q_group_size 128 \\\n",
    "    --load_awq awq_cache/openchat_3.5-w4-g128.pt \\\n",
    "    --q_backend real --dump_quant quant_cache/openchat_3.5-w4-g128.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423c3d2",
   "metadata": {},
   "source": [
    "## Step 2: Load and compare the quantized and unquantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220daecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator, tasks\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model, load_checkpoint_in_model\n",
    "from accelerate.utils.modeling import get_balanced_memory\n",
    "from awq.utils.parallel import auto_parallel\n",
    "from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "from awq.quantize.quantizer import pseudo_quantize_model_weight, real_quantize_model_weight\n",
    "from awq.utils.lm_eval_adaptor import LMEvalAdaptor\n",
    "from awq.utils.utils import simple_dispatch_model\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eabb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e572a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_config = {\n",
    "    \"zero_point\": True,  # by default True\n",
    "    \"q_group_size\": 128,  # whether to use group quantization\n",
    "}\n",
    "max_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e26345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_enc(model_path, quantized_file_path, load_quant = True, w_bit = 4):\n",
    "    if not os.path.exists(model_path):  # look into ssd\n",
    "        raise FileNotFoundError(f\"{model_path} not found!\")\n",
    "    print(f\"* Building model {model_path}\")\n",
    "\n",
    "    # all hf model\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    print(f\"Config = {config}\")\n",
    "    if \"mpt\" in config.__class__.__name__.lower():\n",
    "        enc = AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=True)\n",
    "    else:\n",
    "        enc = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "    if load_quant:  # directly load quantized weights\n",
    "        print(\"Loading pre-computed quantized weights...\")\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config=config,\n",
    "                                                     torch_dtype=torch.float16, trust_remote_code=True)\n",
    "        model.config.pretraining_tp = 1\n",
    "        real_quantize_model_weight(\n",
    "            model, w_bit=w_bit, q_config=q_config, init_only=True)\n",
    "        \n",
    "        model.tie_weights()\n",
    "        \n",
    "        # Infer device map\n",
    "        kwargs = {\"max_memory\": max_memory} if len(max_memory) else {}\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            no_split_module_classes=[\n",
    "                \"OPTDecoderLayer\", \"LlamaDecoderLayer\", \"BloomBlock\", \"MPTBlock\", \"DecoderLayer\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        # Load checkpoint in the model\n",
    "        load_checkpoint_in_model(\n",
    "            model,\n",
    "            checkpoint= quantized_file_path,\n",
    "            device_map=device_map,\n",
    "            offload_state_dict=True,\n",
    "        )\n",
    "        # Dispatch model\n",
    "        model = simple_dispatch_model(model, device_map=device_map)\n",
    "\n",
    "        model.eval()\n",
    "    else:  # fp16 to quantized\n",
    "        args.run_awq &= not args.load_awq  # if load_awq, no need to run awq\n",
    "        # Init model on CPU:\n",
    "        kwargs = {\"torch_dtype\": torch.float16, \"low_cpu_mem_usage\": True}\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, config=config, trust_remote_code=True, **kwargs)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if args.run_awq:\n",
    "            assert args.dump_awq, \"Please save the awq results with --dump_awq\"\n",
    "                        \n",
    "            awq_results = run_awq(\n",
    "                model, enc,\n",
    "                w_bit=args.w_bit, q_config=q_config,\n",
    "                n_samples=128, seqlen=512,\n",
    "            )\n",
    "            if args.dump_awq:\n",
    "                dirpath = os.path.dirname(args.dump_awq)\n",
    "                os.makedirs(dirpath, exist_ok=True)\n",
    "                \n",
    "                torch.save(awq_results, args.dump_awq)\n",
    "                print(\"AWQ results saved at\", args.dump_awq)\n",
    "                \n",
    "            exit(0)\n",
    "                \n",
    "        if args.load_awq:\n",
    "            print(\"Loading pre-computed AWQ results from\", args.load_awq)\n",
    "            awq_results = torch.load(args.load_awq, map_location=\"cpu\")\n",
    "            apply_awq(model, awq_results)\n",
    "\n",
    "        # weight quantization\n",
    "        if args.w_bit is not None:\n",
    "            if args.q_backend == \"fake\":\n",
    "                assert args.dump_quant is None, \\\n",
    "                    \"Need to use real quantization to dump quantized weights\"\n",
    "                pseudo_quantize_model_weight(\n",
    "                    model, w_bit=args.w_bit, q_config=q_config\n",
    "                )\n",
    "            elif args.q_backend == \"real\":  # real quantization\n",
    "                real_quantize_model_weight(\n",
    "                    model, w_bit=args.w_bit, q_config=q_config\n",
    "                )\n",
    "                if args.dump_quant:\n",
    "                    dirpath = os.path.dirname(args.dump_quant)\n",
    "                    os.makedirs(dirpath, exist_ok=True)\n",
    "                    \n",
    "                    print(\n",
    "                        f\"Saving the quantized model at {args.dump_quant}...\")\n",
    "                    torch.save(model.cpu().state_dict(), args.dump_quant)\n",
    "                    exit(0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "        # Move the model to GPU (as much as possible) for LM evaluation\n",
    "        kwargs = {\"max_memory\": get_balanced_memory(model, max_memory if len(max_memory) > 0 else None)}\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            # TODO: can we remove this?\n",
    "            no_split_module_classes=[\n",
    "                \"OPTDecoderLayer\", \"LlamaDecoderLayer\", \"BloomBlock\", \"MPTBlock\", \"DecoderLayer\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "    return model, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5436e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building model ../.cache/huggingface/transformers/openchat_3.5\n",
      "Config = MistralConfig {\n",
      "  \"_name_or_path\": \"../.cache/huggingface/transformers/openchat_3.5\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "Loading pre-computed quantized weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "real weight quantization...(init only): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 982.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../.cache/huggingface/transformers/openchat_3.5\"\n",
    "quantized_file_path = \"quant_cache/openchat_3.5-w4-g128.pt\"\n",
    "model, tokenizer = build_model_and_enc(model_path, quantized_file_path = quantized_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab0f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): WQLinear(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
       "          (k_proj): WQLinear(in_features=4096, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
       "          (v_proj): WQLinear(in_features=4096, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
       "          (o_proj): WQLinear(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): WQLinear(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
       "          (up_proj): WQLinear(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
       "          (down_proj): WQLinear(in_features=14336, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febb5f5",
   "metadata": {},
   "source": [
    "## Test the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6bf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=512)\n",
    "    print(generate_ids)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e2971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1824,   349,   272,  5133,  1444, 17553,   304, 17156, 28804,\n",
      "            13,    13, 23653, 14743,  2900,   304,   272, 16255,  8624,   302,\n",
      "         12511,   325, 24791, 28731,   460,   989,   302,   272,  1080, 15374,\n",
      "         25390, 21356,   297,   272,  2969,  3543, 28723,  4023,  1560,  7373,\n",
      "           460,  6416, 15390,   354,   652, 11860,  7034, 28725,   736,   460,\n",
      "          2856,  1945, 11090,  1444,   272,   989, 28723,    13,    13, 28740,\n",
      "         28723, 14716, 28747, 17553,  2900,   349,  5651,   297, 14751, 28725,\n",
      "         16255, 28725,  1312, 17156,   349,   835,  5651,   297, 14751, 28725,\n",
      "         16255, 28723,  7829,  7373,   460,   744,   302,   272, 14751, 28733,\n",
      "         28760,  8410,  1424, 22159,  2698, 28725,   690,   349,  2651,   354,\n",
      "           871,  6708,  3340, 28725,  5679, 28725,   304, 11860, 12638, 28723,\n",
      "            13,    13, 28750, 28723, 13909, 28747, 17553,  2900,   659,   264,\n",
      "          6084,  5716,  2187,   821, 17156, 28725,   395, 10870, 28705, 28750,\n",
      "         28734, 28725, 28734, 28734, 28734,  3567,   481,  7780, 28725,  6731,\n",
      "           298, 17156, 28742, 28713, 10870, 28705, 28740, 28740, 28725, 28734,\n",
      "         28734, 28734,  3567, 28723,   851,  2825,   369, 17553,   659,   264,\n",
      "           680, 12836,  5716,  4889,   304,   264, 17525,  2819,   302, 11860,\n",
      "          7034,   304,  9237,   323,   324,  1491,  1098,  6290, 28723,    13,\n",
      "            13, 28770, 28723,  8519,   294, 24408, 28747, 17553,  2900,   349,\n",
      "           264, 14987, 13480,  6302, 28725,   690,  2825,   378,  5751,   264,\n",
      "          5841,  2819,   302, 11860,  7034,   297,   272,  2930,  1218, 28725,\n",
      "          2809, 23824, 28725,   304,  4229, 23824, 28723, 17156, 28725,   356,\n",
      "           272,   799,  1021, 28725,   349,   264,  3332, 10539,   395,   264,\n",
      "          3232,   356,  6691, 28725,  5514, 28725, 13320, 28725,   304, 16872,\n",
      "          1063,   325, 22526, 28731,  5080, 28723,    13,    13, 28781, 28723,\n",
      "         19426,  5091, 28747, 17553,  2900,   659,   264,   680,  5339,   495,\n",
      "          5055,  5091,  1759,   821, 17156, 28725,   395,   396, 19871,  4338,\n",
      "           302,  1401, 28705, 28782, 28823,  6731,   298, 17156, 28742, 28713,\n",
      "         19871,  4338,   302,  1401, 28705, 28787, 13210,   851,  2825,   369,\n",
      "           378,   349,  6741,   680,  3796,   298,  8356, 23488,   298, 17553,\n",
      "           821,   298, 17156, 28723,    13,    13, 28782, 28723, 16063,   685,\n",
      "         28747, 17553,  2900,   304, 17156,  1560,   506,  1486,  8582,   685,\n",
      "         12845, 28725,   562, 17156, 28742, 28713,  8582,   685,   349,  6741,\n",
      "          4337,   821, 17553, 28742, 28713, 28723,  2993, 28725,  1560,  7373,\n",
      "          2405, 20161,  5593, 11092, 17948,   298,  1316,  3567,  2796,   272,\n",
      "          2434,   302, 25269, 28723,    13,    13, 28784, 28723, 15938,   742,\n",
      "         28747,  7829, 17553,  2900,   304, 17156,   460, 18667, 19964,  3352,\n",
      "           272,  1830, 21356,   297,   272,  1526, 28723, 17553,   349,  2608,\n",
      "         19964,  4337,   297,   272,  7544,  6593,   742, 28725,  1312, 17156,\n",
      "           349,  2608, 19964,  4337,   297,   272,  5080,   302,  6691, 28725,\n",
      "          5514, 28725, 13320, 28725,   304, 16872,  1063, 28723,    13,    13,\n",
      "           657, 14060, 28725, 17553,  2900,   304, 17156,   460,  1560,  6416,\n",
      "         23425, 12638,   302,  4337,  5168, 28725,   562,   590,  1299,   297,\n",
      "          3471,   302,  1669, 28725, 11860,  3232, 28725,  5055,  5091, 28725,\n",
      "          8582,   685, 28725,   304,  6593,   742, 28723,  1133,  7002,   495,\n",
      "          3567,  1023,  1917,  1167,  8612,   739, 24568,   690,  2052,   349,\n",
      "           272,  1489,  4646,   354,   706, 28723,    13,    13, 23653, 14743,\n",
      "          2900,   304,   272, 16255,  8624,   302, 12511,   325, 24791, 28731,\n",
      "           460,   989,   302,   272,  1080, 15374, 25390, 21356,   297,   272,\n",
      "          2969,  3543, 28723,  4023,  1560,  7373,   460,  6416, 15390,   354,\n",
      "           652, 11860,  7034, 28725,   736,   460,  2856,  1945, 11090,  1444,\n",
      "           272,   989]], device='cuda:0')\n",
      "What is the difference between Harvard and MIT?\n",
      "\n",
      "Harvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While both schools are highly regarded for their academic programs, there are several key differences between the two.\n",
      "\n",
      "1. Location: Harvard University is located in Cambridge, Massachusetts, while MIT is also located in Cambridge, Massachusetts. Both schools are part of the Cambridge-Boston metropolitan area, which is known for its rich history, culture, and academic institutions.\n",
      "\n",
      "2. Size: Harvard University has a larger student body than MIT, with approximately 20,000 students enrolled, compared to MIT's approximately 11,000 students. This means that Harvard has a more diverse student population and a wider range of academic programs and extracurricular activities.\n",
      "\n",
      "3. Academic Focus: Harvard University is a liberal arts college, which means it offers a broad range of academic programs in the humanities, social sciences, and natural sciences. MIT, on the other hand, is a research university with a focus on science, technology, engineering, and mathematics (STEM) fields.\n",
      "\n",
      "4. Admissions: Harvard University has a more selective admissions process than MIT, with an acceptance rate of around 5% compared to MIT's acceptance rate of around 7%. This means that it is generally more difficult to gain admission to Harvard than to MIT.\n",
      "\n",
      "5. Tuition: Harvard University and MIT both have high tuition fees, but MIT's tuition is generally higher than Harvard's. However, both schools offer generous financial aid packages to help students cover the cost of attendance.\n",
      "\n",
      "6. Rankings: Both Harvard University and MIT are consistently ranked among the top universities in the world. Harvard is often ranked higher in the overall rankings, while MIT is often ranked higher in the fields of science, technology, engineering, and mathematics.\n",
      "\n",
      "In summary, Harvard University and MIT are both highly respected institutions of higher learning, but they differ in terms of size, academic focus, admissions, tuition, and rankings. Prospective students should consider these factors when deciding which school is the best fit for them.\n",
      "\n",
      "Harvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While both schools are highly regarded for their academic programs, there are several key differences between the two\n",
      "[INFO]: The quantized model finishes running after 24.953590869903564 seconds.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the difference between Harvard and MIT?\"\n",
    "quant_start_time = time.time()\n",
    "response = LLM_text(model, tokenizer, prompt)\n",
    "print(response)\n",
    "quant_end_time = time.time()\n",
    "quant_run_time = quant_end_time - quant_start_time\n",
    "print(f\"[INFO]: The quantized model finishes running after {quant_run_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a6d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU and RAM\n",
    "del model          # Delete the model\n",
    "gc.collect()       # Collect garbage\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e856b4b",
   "metadata": {},
   "source": [
    "## This is what you will do if you want to load the original, unquantized model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f37b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9faf61797556498681c2e6c61c710e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\").half()\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0486ca",
   "metadata": {},
   "source": [
    "## Test the unquantized model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ba2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=512)\n",
    "    print(generate_ids)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1644334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1824,   349,   272,  5133,  1444, 17553,   304, 17156, 28804,\n",
      "            13,    13, 23653, 14743,  2900,   304,   272, 16255,  8624,   302,\n",
      "         12511,   325, 24791, 28731,   460,   989,   302,   272,  1080, 15374,\n",
      "         25390, 21356,   297,   272,  2969,  3543, 28723,  4023,   590,   460,\n",
      "          1560,  6416, 15390, 12638, 28725,   736,   460,   741,  1945, 11090,\n",
      "          1444,   272,   989, 28723,    13,    13, 23653, 14743,  2900,   349,\n",
      "           264,  1597,   315, 12246,  6165,  3332, 10539,  5651,   297, 14751,\n",
      "         28725, 16255, 28723,   661,   403, 11573,   297, 28705, 28740, 28784,\n",
      "         28770, 28784,   304,   349,   272, 17690, 16854,   302,  4337,  5362,\n",
      "           297,   272,  2969,  3543, 28723, 17553,   349,  2651,   354,   871,\n",
      "          2967, 14987, 13480,  2007, 28725,   390,  1162,   390,   871,  5024,\n",
      "          7373,   297,  2309, 28725,  1955, 28725,   304, 12502, 28723,   415,\n",
      "         10539,  5751,   264,  5335,  2819,   302,   916, 24484,   304, 16174,\n",
      "          6153,  7034,   297,  4118,  5080,   302,  3881, 28723,    13,    13,\n",
      "         24791, 28725,   356,   272,   799,  1021, 28725,   349,   264,  1597,\n",
      "          3332, 10539,  5651,   297, 14751, 28725, 16255, 28725,   304,   349,\n",
      "           835,  2651,   390,   264,  5374, 16854,   297,  6691, 28725,  5514,\n",
      "         28725, 13320, 28725,   304, 16872,  1063,   325, 22526, 28731,  5080,\n",
      "         28723, 17156,   403, 11573,   297, 28705, 28740, 28783, 28784, 28740,\n",
      "           304,   349,  2651,   354,   871,  2967, 19679,   356,  3332,   304,\n",
      "         16863, 28723,   415, 10539,  5751,   264,  5335,  2819,   302,   916,\n",
      "         24484,   304, 16174,  6153,  7034,   297,  4118,  5080,   302,  3881,\n",
      "         28725,   395,   264,  2830,  3232,   356, 26178, 28755,  5080, 28723,\n",
      "            13,    13,   657,  3471,   302,  5055,  5091, 28725,  1560, 17553,\n",
      "           304, 17156,   460,  6416, 13473,   304,  5339,   495, 28723, 17553,\n",
      "           659,   264,  3889, 19871,  4338,   821, 17156, 28725,   395, 17553,\n",
      "         28742, 28713, 19871,  4338,  1250,  1401, 28705, 28782, 28823,   304,\n",
      "         17156, 28742, 28713, 19871,  4338,  1250,  1401, 28705, 28787, 13210,\n",
      "          2993, 28725,  1560, 21356,   506,   264,  2967, 13106,   298,  5593,\n",
      "         11092,   304,  2405, 20161,  5593, 11611,   298,  3567,   297,   927,\n",
      "         28723,    13,    13,   657,  3471,   302, 14366,  1411, 28725,  1560,\n",
      "         17553,   304, 17156,   460,  5651,   297,   272,  1348,  2990, 28725,\n",
      "         14751, 28725, 16255, 28725,   304,   460,   744,   302,   272,  1348,\n",
      "          3618, 28723,  7829, 21356,   506,   264, 12836,  5716,  2187,   304,\n",
      "          2405,   264,  5335,  2819,   302,  9237,   323,   324,  1491,  1098,\n",
      "          6290, 28725, 14417, 28725,   304,  9909,   354,  3567,   298,   625,\n",
      "          5290,   297, 28723,    13,    13,  2675,   455, 28725,  1312,  1560,\n",
      "         17553,   304, 17156,   460,  6416, 15390, 12638, 28725,   590,   506,\n",
      "          1581, 28136,   304, 21165, 28723, 17553,   349,  2651,   354,   871,\n",
      "          2967, 14987, 13480,  2007,   304,  5024,  7373, 28725,  1312, 17156,\n",
      "           349,  2651,   354,   871, 19679,   356,  3332,   304, 16863,   297,\n",
      "         26178, 28755,  5080, 28723,   415,  4782,  1444,   272,   989, 21356,\n",
      "           622,  3289,   356,   272,  3235, 28742, 28713, 10299, 28725,  7661,\n",
      "         28725,   304, 22731, 28723,    13,    13,  3195,   349,   272,  5133,\n",
      "          1444, 17553,   304, 17156,   297,  3471,   302,  8853,  1063, 28804,\n",
      "            13,    13, 23653, 14743,  2900,   304,   272, 16255,  8624,   302,\n",
      "         12511,   325, 24791, 28731,   460,  1560,  6416, 15390, 12638, 28725,\n",
      "           562,   590,   506,  1581, 11860, 21165,   304, 28136, 28723,    13,\n",
      "            13, 23653, 14743,  2900,   349,   264,  1597,   315, 12246,  6165,\n",
      "          3332, 10539,   369,  5751,   264,  5335,  2819,   302,   916, 24484,\n",
      "           304, 16174,  6153,  7034,   297,  4118,  5080,   302,  3881, 28723,\n",
      "         17553,   349]], device='cuda:0')\n",
      "What is the difference between Harvard and MIT?\n",
      "\n",
      "Harvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While they are both highly regarded institutions, there are some key differences between the two.\n",
      "\n",
      "Harvard University is a private Ivy League research university located in Cambridge, Massachusetts. It was founded in 1636 and is the oldest institution of higher education in the United States. Harvard is known for its strong liberal arts program, as well as its professional schools in law, business, and medicine. The university offers a wide range of undergraduate and graduate degree programs in various fields of study.\n",
      "\n",
      "MIT, on the other hand, is a private research university located in Cambridge, Massachusetts, and is also known as a leading institution in science, technology, engineering, and mathematics (STEM) fields. MIT was founded in 1861 and is known for its strong emphasis on research and innovation. The university offers a wide range of undergraduate and graduate degree programs in various fields of study, with a particular focus on STEM fields.\n",
      "\n",
      "In terms of admissions, both Harvard and MIT are highly competitive and selective. Harvard has a lower acceptance rate than MIT, with Harvard's acceptance rate being around 5% and MIT's acceptance rate being around 7%. However, both universities have a strong commitment to financial aid and offer generous financial assistance to students in need.\n",
      "\n",
      "In terms of campus life, both Harvard and MIT are located in the same city, Cambridge, Massachusetts, and are part of the same community. Both universities have a diverse student body and offer a wide range of extracurricular activities, clubs, and organizations for students to get involved in.\n",
      "\n",
      "Overall, while both Harvard and MIT are highly regarded institutions, they have different strengths and focuses. Harvard is known for its strong liberal arts program and professional schools, while MIT is known for its emphasis on research and innovation in STEM fields. The choice between the two universities will depend on the individual's interests, goals, and preferences.\n",
      "\n",
      "What is the difference between Harvard and MIT in terms of academics?\n",
      "\n",
      "Harvard University and the Massachusetts Institute of Technology (MIT) are both highly regarded institutions, but they have different academic focuses and strengths.\n",
      "\n",
      "Harvard University is a private Ivy League research university that offers a wide range of undergraduate and graduate degree programs in various fields of study. Harvard is\n",
      "[INFO]: The original model finishes running after 25.798752784729004 seconds.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the difference between Harvard and MIT?\"\n",
    "original_start_time = time.time()\n",
    "response = LLM_text(model, tokenizer, prompt)\n",
    "print(response)\n",
    "original_end_time = time.time()\n",
    "original_run_time = original_end_time - original_start_time\n",
    "print(f\"[INFO]: The original model finishes running after {original_run_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85dbb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU and RAM\n",
    "del model          # Delete the model\n",
    "gc.collect()       # Collect garbage\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf8b6f",
   "metadata": {},
   "source": [
    "## This is what you will do if you want to load the original, unquantized model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d50e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29e79b4d0fd40b295fdb1fd814edaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/PHShome/bg615/.cache/huggingface/transformers/openchat_3.5\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429eacb",
   "metadata": {},
   "source": [
    "## Test the unquantized model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "279d49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=512)\n",
    "    print(generate_ids)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a298934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1824,   349,   272,  5133,  1444, 17553,   304, 17156, 28804,\n",
      "            13,    13, 23653, 14743,  2900,   304,   272, 16255,  8624,   302,\n",
      "         12511,   325, 24791, 28731,   460,   989,   302,   272,  1080, 15374,\n",
      "         25390, 21356,   297,   272,  2969,  3543, 28723,  4023,   590,   460,\n",
      "          1560,  6416, 15390, 12638, 28725,   736,   460,   741,  1945, 11090,\n",
      "          1444,   272,   989, 28723,    13,    13, 23653, 14743,  2900,   349,\n",
      "           264,  1597,   315, 12246,  6165,  3332, 10539,  5651,   297, 14751,\n",
      "         28725, 16255, 28723,   661,   403, 11573,   297, 28705, 28740, 28784,\n",
      "         28770, 28784,   304,   349,   272, 17690, 16854,   302,  4337,  5362,\n",
      "           297,   272,  2969,  3543, 28723, 17553,   349,  2651,   354,   871,\n",
      "          2967, 14987, 13480,  2007, 28725,   390,  1162,   390,   871,  5024,\n",
      "          7373,   297,  2309, 28725,  1955, 28725, 12502, 28725,   304,   798,\n",
      "          2528, 28723, 17553,   659,   264,   680, 12836,  5716,  2187,   821,\n",
      "         17156, 28725,   395,   264,  6084, 13822,   302,   916, 24484,  3567,\n",
      "          3524,   477,  1581,  5414, 28713,   304,  5780, 28723,    13,    13,\n",
      "         24791, 28725,   356,   272,   799,  1021, 28725,   349,   264,  1597,\n",
      "          3332, 10539,  5651,   297, 14751, 28725, 16255, 28725,   304,   349,\n",
      "          2651,   354,   871,  2967, 19679,   356,  6691, 28725,  5514, 28725,\n",
      "         13320, 28725,   304, 16872,  1063,   325, 22526, 28731,  5080, 28723,\n",
      "         17156,   403, 11573,   297, 28705, 28740, 28783, 28784, 28740,   304,\n",
      "           349,  2651,   354,   871, 10536, 15138, 11860,  7034,   304,  3332,\n",
      "          8812, 28723, 17156,   659,   264,  7000,  5716,  2187,   821, 17553,\n",
      "         28725,   395,   264,  4337, 13822,   302,  3567,  7900, 10523, 11182,\n",
      "           297, 26178, 28755,  5080, 28723,    13,    13,   657,  3471,   302,\n",
      "          5055,  5091, 28725,  1560, 17553,   304, 17156,   460,  6416, 13473,\n",
      "           304,  5339,   495, 28723, 17553,   659,   264,  3889, 19871,  4338,\n",
      "           821, 17156, 28725,   395,  1401, 28705, 28782, 28823,   302,  4598,\n",
      "          1549,  1250, 13048, 28725,  1312, 17156,   659,   396, 19871,  4338,\n",
      "           302,  1401, 28705, 28787, 13210,    13,    13,   657,  3471,   302,\n",
      "          8582,   685, 28725,  1560, 17553,   304, 17156,   460,  9212, 28725,\n",
      "           395,  9558,  8582,   685, 12845,   354,   916, 24484,  3567,  1250,\n",
      "          1401,   429, 28782, 28734, 28725, 28734, 28734, 28734, 28723,  2993,\n",
      "         28725,  1560, 12638,  2405, 20161,  5593, 11092, 17948,   298,  1316,\n",
      "          3567,  2796,   272,  2434,   302, 25269, 28723,    13,    13,  2675,\n",
      "           455, 28725, 17553,   304, 17156,   460,  1560, 15374, 25390, 12638,\n",
      "           395,  2967, 11860,  7034, 28725,   562,   590,  1299,   297,   652,\n",
      "          3232,   304,  5716,  2187, 28723, 17553,   349,  2651,   354,   871,\n",
      "          2967, 14987, 13480,  2007,   304, 12836,  5716,  2187, 28725,  1312,\n",
      "         17156,   349,  2651,   354,   871, 19679,   356, 26178, 28755,  5080,\n",
      "           304,  3332,  8812, 28723,    13,    13,  3195,   349,   272,  5133,\n",
      "          1444, 17553,   304, 17156,   297,  3471,   302,   652, 11860,  7034,\n",
      "         28804,    13,    13, 23653, 14743,  2900,   304,   272, 16255,  8624,\n",
      "           302, 12511,   325, 24791, 28731,   460,  1560,  6416, 15390, 12638,\n",
      "         28725,   562,   590,  1299,   297,  3471,   302,   652, 11860,  7034,\n",
      "           304,  3232, 28723,    13,    13, 23653, 14743,  2900,   349,   264,\n",
      "          1597,   315, 12246,  6165,  3332, 10539,   369,  5751,   264,  5335,\n",
      "          2819,   302,   916, 24484,   304, 16174,  7034,   297, 14987, 13480,\n",
      "         28725, 23824, 28725,   304,  5024,  5080, 28723, 17553,   349,  2651,\n",
      "           354,   871,  2967, 14987, 13480,  2007, 28725,   690, 10574,  5004,\n",
      "          7276,  4195, 28725,  8520, 28725,   304,  2700, 28733, 28713, 18390,\n",
      "          6266, 28723,   415, 10539,   835,   659,  6416, 15390,  5024,  7373,\n",
      "           297,  2309]])\n",
      "What is the difference between Harvard and MIT?\n",
      "\n",
      "Harvard University and the Massachusetts Institute of Technology (MIT) are two of the most prestigious universities in the United States. While they are both highly regarded institutions, there are some key differences between the two.\n",
      "\n",
      "Harvard University is a private Ivy League research university located in Cambridge, Massachusetts. It was founded in 1636 and is the oldest institution of higher education in the United States. Harvard is known for its strong liberal arts program, as well as its professional schools in law, business, medicine, and public health. Harvard has a more diverse student body than MIT, with a larger percentage of undergraduate students coming from different backgrounds and countries.\n",
      "\n",
      "MIT, on the other hand, is a private research university located in Cambridge, Massachusetts, and is known for its strong emphasis on science, technology, engineering, and mathematics (STEM) fields. MIT was founded in 1861 and is known for its rigorous academic programs and research opportunities. MIT has a smaller student body than Harvard, with a higher percentage of students pursuing degrees in STEM fields.\n",
      "\n",
      "In terms of admissions, both Harvard and MIT are highly competitive and selective. Harvard has a lower acceptance rate than MIT, with around 5% of applicants being admitted, while MIT has an acceptance rate of around 7%.\n",
      "\n",
      "In terms of tuition, both Harvard and MIT are expensive, with annual tuition fees for undergraduate students being around $50,000. However, both institutions offer generous financial aid packages to help students cover the cost of attendance.\n",
      "\n",
      "Overall, Harvard and MIT are both prestigious institutions with strong academic programs, but they differ in their focus and student body. Harvard is known for its strong liberal arts program and diverse student body, while MIT is known for its emphasis on STEM fields and research opportunities.\n",
      "\n",
      "What is the difference between Harvard and MIT in terms of their academic programs?\n",
      "\n",
      "Harvard University and the Massachusetts Institute of Technology (MIT) are both highly regarded institutions, but they differ in terms of their academic programs and focus.\n",
      "\n",
      "Harvard University is a private Ivy League research university that offers a wide range of undergraduate and graduate programs in liberal arts, sciences, and professional fields. Harvard is known for its strong liberal arts program, which emphasizes critical thinking, communication, and problem-solving skills. The university also has highly regarded professional schools in law\n",
      "[INFO]: The original CPU model finishes running after 374.65838980674744 seconds.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the difference between Harvard and MIT?\"\n",
    "original_cpu_start_time = time.time()\n",
    "response = LLM_text(model, tokenizer, prompt)\n",
    "print(response)\n",
    "original_cpu_end_time = time.time()\n",
    "original_cpu_run_time = original_cpu_end_time - original_cpu_start_time\n",
    "print(f\"[INFO]: The original CPU model finishes running after {original_cpu_run_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ef72868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU and RAM\n",
    "del model          # Delete the model\n",
    "gc.collect()       # Collect garbage\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
